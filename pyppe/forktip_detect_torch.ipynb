{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# object key point detection with RCNN\n",
    "# 1. load dependent packages for pytorch\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.transforms import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset class\n",
    "class classdataset(Dataset):\n",
    "    def __init__(self, root, transform=None, demo=False):\n",
    "        self.root = root # root path\n",
    "        self.transform = transform\n",
    "        self.image_files = sorted(os.listdir(os.path.join(root, \"images\"))) # image path\n",
    "        self.annotation_files = sorted(os.listdir(os.path.join(root, \"annotation\"))) # anotation path\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = os.path.join(self.root, \"images\", self.image_files[index])\n",
    "        annotation_path = os.path.join(self.root, \"anotation\", self.annotation_files[index])\n",
    "\n",
    "        image_src = cv2.imread(image_path)\n",
    "        image_src = cv2.cvtColor(image_src, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        with open(annotation_path) as f:\n",
    "            data = json.load(f)\n",
    "            bbox_original = data['bboxes']\n",
    "            keypoint_original = data['keypoints']\n",
    "            print(bbox_original, keypoint_original)\n",
    "        \n",
    "        # converting keypoint from [x,y,vis] to [x,y] format and flatting nested list\n",
    "        if self.transform:\n",
    "            pass\n",
    "        \n",
    "        img, bboxes, keypoints = image_src, bbox_original, keypoint_original\n",
    "\n",
    "        # convert into a torch tensor\n",
    "        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)\n",
    "        target = {}\n",
    "        target[\"boxes\"] = bboxes\n",
    "        target[\"labels\"] = torch.as_tensor([1 for _ in bboxes], dtype=torch.int64)\n",
    "        target[\"image_id\"] = torch.tensor([index])\n",
    "        target[\"area\"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
    "        target[\"iscrowd\"] = torch.zeros(len(bboxes), dtype=torch.int64)\n",
    "        target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32)        \n",
    "        img = F.to_tensor(img)\n",
    "\n",
    "        bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)\n",
    "        target_original = {}\n",
    "        target_original[\"boxes\"] = bboxes_original\n",
    "        target_original[\"labels\"] = torch.as_tensor([1 for _ in bboxes_original], dtype=torch.int64) # all objects are glue tubes\n",
    "        target_original[\"image_id\"] = torch.tensor([index])\n",
    "        target_original[\"area\"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (bboxes_original[:, 2] - bboxes_original[:, 0])\n",
    "        target_original[\"iscrowd\"] = torch.zeros(len(bboxes_original), dtype=torch.int64)\n",
    "        target_original[\"keypoints\"] = torch.as_tensor(keypoint_original, dtype=torch.float32)        \n",
    "        img_original = F.to_tensor(img_original)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2801148153.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    import .transforms, .utils, .engine, .train\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import transforms, utils, engine, train\n",
    "from utils import collate_fn\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "# use accelerator if it is available (mps for Mac)\n",
    "device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n",
    "\n",
    "_keypoint_folder_train = 'fork_dataset/train'\n",
    "_keypoint_folder_test = 'fork_dastaset/test'\n",
    "\n",
    "dataset = classdataset(_keypoint_folder_train)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
